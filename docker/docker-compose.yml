networks:
  internal:
    driver: bridge


services:
  analyzer:
    build:
      dockerfile: docker/analyzer.dockerfile
      context: ..
    networks:
      - internal

  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    environment:
      # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
      LLAMA_ARG_MODEL_URL: mistralai/Mixtral-8x7B-v0.1
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_N_PARALLEL: 2
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
#  language_processing-server:
#    build:
#      dockerfile: language_processing.dockerfile
#      context: ..
#    networks:
#      - internal
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: 1
#              capabilities: [gpu]
#